{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "Malware-Images-training.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "gXDkcGkoB1gU",
        "cFzEIvF_B1gX",
        "jb7YwsK1B1gX"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUISRDwHB1gG"
      },
      "source": [
        "### This notebook contains the training for all the models, on the processed dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2021-12-05T18:52:27.413627Z",
          "iopub.execute_input": "2021-12-05T18:52:27.414436Z",
          "iopub.status.idle": "2021-12-05T18:52:34.612102Z",
          "shell.execute_reply.started": "2021-12-05T18:52:27.414390Z",
          "shell.execute_reply": "2021-12-05T18:52:34.611126Z"
        },
        "trusted": true,
        "id": "wcTgTH9HB1gH"
      },
      "source": [
        "### importing the required libraries\n",
        "! pip install torchsummary\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.metrics import *\n",
        "import sys\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary\n",
        "#import warnings\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import warnings\n",
        "from typing import Type, Any, Callable, Union, List, Optional\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import torch\n",
        "from torchvision import datasets, transforms, models\n",
        "import torchvision\n",
        "from tqdm import tqdm\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "import tensorflow as tf\n",
        "from sklearn.utils import shuffle\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": false,
        "_kg_hide-output": false,
        "execution": {
          "iopub.status.busy": "2021-12-05T18:52:34.615876Z",
          "iopub.execute_input": "2021-12-05T18:52:34.616155Z",
          "iopub.status.idle": "2021-12-05T18:52:34.621955Z",
          "shell.execute_reply.started": "2021-12-05T18:52:34.616123Z",
          "shell.execute_reply": "2021-12-05T18:52:34.620796Z"
        },
        "trusted": true,
        "id": "p1cnUVogB1gI"
      },
      "source": [
        "## function for calculating the scores\n",
        "from sklearn.metrics import accuracy_score\n",
        "def f1_score_(actual, predicted):\n",
        "  score_ = f1_score(actual, predicted, average=\"weighted\")\n",
        "  matrix = confusion_matrix(actual,predicted)\n",
        "  return score_,matrix\n",
        "\n",
        "def calculate_accuracy(actual, predicted):\n",
        "  return accuracy_score(actual,predicted)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJJd5jI-B1gK"
      },
      "source": [
        "### The following function, import the dataset, divide that into test and train, normalize the pixels and convert to tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-05T18:52:34.624028Z",
          "iopub.execute_input": "2021-12-05T18:52:34.624329Z",
          "iopub.status.idle": "2021-12-05T18:52:34.634957Z",
          "shell.execute_reply.started": "2021-12-05T18:52:34.624292Z",
          "shell.execute_reply": "2021-12-05T18:52:34.634117Z"
        },
        "trusted": true,
        "id": "UbfGSPz8B1gK"
      },
      "source": [
        "class MalwareData(Dataset):\n",
        "  def __init__(self,path,task):\n",
        "    df = pd.read_csv(path)\n",
        "    X = df.iloc[:,:-1]/255.0\n",
        "    Y = df.iloc[:,-1]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42,shuffle=True)\n",
        "\n",
        "    if task == \"train\":\n",
        "      self.x = torch.from_numpy(X_train.values)\n",
        "      self.y = torch.from_numpy(y_train.values)\n",
        "    else:\n",
        "      self.x = torch.from_numpy(X_test.values)\n",
        "      self.y = torch.from_numpy(y_test.values)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.y.shape[0]\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return (self.x[idx] , self.y[idx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-05T18:52:34.637229Z",
          "iopub.execute_input": "2021-12-05T18:52:34.638010Z",
          "iopub.status.idle": "2021-12-05T18:52:39.833603Z",
          "shell.execute_reply.started": "2021-12-05T18:52:34.637967Z",
          "shell.execute_reply": "2021-12-05T18:52:39.832774Z"
        },
        "trusted": true,
        "id": "RkgUEeoMB1gL"
      },
      "source": [
        "## Loading the dataset and dividing into the batch of 16 the train set\n",
        "Oversampled_path = \"../input/malwareprojectmulitdata/FINAL_DATASET.csv\"\n",
        "Batch = 16\n",
        "data_tr = MalwareData(Oversampled_path,\"train\")\n",
        "data_train = DataLoader(dataset = data_tr, batch_size = Batch, shuffle =True)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "data_ts = MalwareData(Oversampled_path,\"test\")\n",
        "data_test = DataLoader(dataset = data_ts, batch_size = len(data_ts), shuffle =True)\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-05T18:52:39.835420Z",
          "iopub.execute_input": "2021-12-05T18:52:39.835747Z",
          "iopub.status.idle": "2021-12-05T18:52:39.840474Z",
          "shell.execute_reply.started": "2021-12-05T18:52:39.835708Z",
          "shell.execute_reply": "2021-12-05T18:52:39.839643Z"
        },
        "trusted": true,
        "id": "gu05cDZnB1gM"
      },
      "source": [
        "## Adding the column names\n",
        "columns_names = list(df.columns)\n",
        "columns_names.remove('label')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-05T18:52:39.841967Z",
          "iopub.execute_input": "2021-12-05T18:52:39.842480Z",
          "iopub.status.idle": "2021-12-05T18:52:49.421916Z",
          "shell.execute_reply.started": "2021-12-05T18:52:39.842443Z",
          "shell.execute_reply": "2021-12-05T18:52:49.421170Z"
        },
        "trusted": true,
        "id": "JKmNqLhWB1gN"
      },
      "source": [
        "## saving the test data for further evaluation\n",
        "for x , y in data_test:\n",
        "    test_data = pd.DataFrame(np.array(x),columns = columns_names)\n",
        "    test_data['label']=y    \n",
        "test_data.to_csv(\"./test-data.csv\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dX5cgQ_lB1gN"
      },
      "source": [
        "### ------------------The training starts from here------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-05T18:52:49.423819Z",
          "iopub.execute_input": "2021-12-05T18:52:49.424195Z",
          "iopub.status.idle": "2021-12-05T18:52:49.429413Z",
          "shell.execute_reply.started": "2021-12-05T18:52:49.424152Z",
          "shell.execute_reply": "2021-12-05T18:52:49.428755Z"
        },
        "trusted": true,
        "id": "5W8w1PmmB1gO"
      },
      "source": [
        "##defining our training function\n",
        "def train(model, x, y, optimizer, criterion):\n",
        "    #forward\n",
        "    optimizer.zero_grad()\n",
        "    output = model(x)\n",
        "    loss = criterion(output,y)\n",
        "    ##backward\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss, output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-05T18:52:49.430588Z",
          "iopub.execute_input": "2021-12-05T18:52:49.432092Z",
          "iopub.status.idle": "2021-12-05T18:52:49.440290Z",
          "shell.execute_reply.started": "2021-12-05T18:52:49.432039Z",
          "shell.execute_reply": "2021-12-05T18:52:49.439512Z"
        },
        "trusted": true,
        "id": "JYqfcxWRB1gP"
      },
      "source": [
        "## for saving the model dump files for further evaluation\n",
        "def save_model(model_,model_name):\n",
        "    model_file = \"./\"+ str(model_name) +'.pth'\n",
        "    torch.save(model_.state_dict(),model_file)\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsCgQgLVB1gP"
      },
      "source": [
        "### Deep neural networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-05T18:52:49.441565Z",
          "iopub.execute_input": "2021-12-05T18:52:49.441949Z",
          "iopub.status.idle": "2021-12-05T18:52:49.450648Z",
          "shell.execute_reply.started": "2021-12-05T18:52:49.441914Z",
          "shell.execute_reply": "2021-12-05T18:52:49.449929Z"
        },
        "trusted": true,
        "id": "ZYm5pC78B1gQ"
      },
      "source": [
        "##DNNs\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DNN_Net(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "      super(DNN_Net, self).__init__()\n",
        "      self.network = nn.Sequential(\n",
        "                    nn.Linear(1024, 128), ##layer 1\n",
        "                    nn.BatchNorm1d(128),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(128,64), ## layer 2\n",
        "                    nn.BatchNorm1d(64),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(64, 8),  ## output layer\n",
        "    )                \n",
        "    def forward(self, x):\n",
        "      x = self.network(x)\n",
        "      return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-05T05:52:25.474379Z",
          "iopub.execute_input": "2021-12-05T05:52:25.474750Z",
          "iopub.status.idle": "2021-12-05T05:54:02.585579Z",
          "shell.execute_reply.started": "2021-12-05T05:52:25.474716Z",
          "shell.execute_reply": "2021-12-05T05:54:02.584855Z"
        },
        "trusted": true,
        "id": "QriigR5RB1gQ"
      },
      "source": [
        "### DNN Training\n",
        "epochs = 40\n",
        "loss_list = []\n",
        "\n",
        "model_DNN = DNN_Net().\n",
        "optimizer = torch.optim.Adam(model_DNN.parameters(),lr=0.001)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  loss_ = 0\n",
        "  for i, (data, labels) in tqdm(enumerate(data_train)):\n",
        "    x_train = data.float().to(device)\n",
        "    y_train = labels.long().to(device)\n",
        "    #print(x_train.shape)\n",
        "    loss, output = train(model_DNN , x_train,y_train,optimizer,criterion)\n",
        "    loss_ += loss.item()\n",
        "    \n",
        "  print(\"Epoch {}, Training loss:{}\".format(epoch, loss_ / len(data_train)))\n",
        "  loss_list.append(loss_ / len(data_train))\n",
        "\n",
        "save_model(model_DNN,\"Simple-DNN\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Czp3V2vB1gQ"
      },
      "source": [
        "### Convolutional Neural Networks - custom"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-05T18:53:55.806102Z",
          "iopub.execute_input": "2021-12-05T18:53:55.806633Z",
          "iopub.status.idle": "2021-12-05T18:53:55.818453Z",
          "shell.execute_reply.started": "2021-12-05T18:53:55.806595Z",
          "shell.execute_reply": "2021-12-05T18:53:55.817481Z"
        },
        "trusted": true,
        "id": "SSSfZDwpB1gQ"
      },
      "source": [
        "####### Convolutional Nets\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNN_Net(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(CNN_Net, self).__init__()\n",
        "      self.CNN1 = nn.Sequential(\n",
        "                     nn.Conv2d(1, 16, 2, 2, 2),     \n",
        "                     nn.ReLU(inplace=True),  \n",
        "                     nn.BatchNorm2d(16),\n",
        "                     nn.Dropout(0.2),                    \n",
        "                     nn.MaxPool2d(2), \n",
        "                    )  \n",
        "      self.CNN2 = nn.Sequential(\n",
        "                     nn.Conv2d(16, 32, 2, 2, 2),     \n",
        "                     nn.ReLU(inplace=True), \n",
        "                     nn.BatchNorm2d(32),\n",
        "                     nn.Dropout(0.2),                      \n",
        "                     nn.MaxPool2d(2), \n",
        "                    )   \n",
        "      self.CNN3 = nn.Sequential(\n",
        "                     nn.Conv2d(32, 16, 2, 1, 2),     \n",
        "                     nn.ReLU(inplace=True),\n",
        "                     nn.BatchNorm2d(16),\n",
        "                     nn.Dropout(0.2),                      \n",
        "                     nn.MaxPool2d(2), \n",
        "                    )   \n",
        "      self.FCC = nn.Sequential(\n",
        "                    nn.Linear(144, 64), ##layer 1\n",
        "                    nn.ReLU(inplace=True),\n",
        "                    nn.Linear(64, 32), ##layer 1\n",
        "                    nn.ReLU(inplace=True),\n",
        "                    nn.Linear(32, 8),##output layer\n",
        "                      \n",
        "    )                              \n",
        "    def forward(self, x):\n",
        "      x = self.CNN1(x)\n",
        "      x = self.CNN2(x)\n",
        "      x = self.CNN3(x)\n",
        "      x = x.reshape(x.shape[0],-1)\n",
        "      #print(x.shape)\n",
        "      x = self.FCC(x)\n",
        "      return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-05T18:54:05.373030Z",
          "iopub.execute_input": "2021-12-05T18:54:05.373723Z",
          "iopub.status.idle": "2021-12-05T18:54:09.888634Z",
          "shell.execute_reply.started": "2021-12-05T18:54:05.373690Z",
          "shell.execute_reply": "2021-12-05T18:54:09.887832Z"
        },
        "trusted": true,
        "id": "gNf0outnB1gR"
      },
      "source": [
        "model = CNN_Net().to(device)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-05T03:27:46.112576Z",
          "iopub.execute_input": "2021-12-05T03:27:46.112885Z",
          "iopub.status.idle": "2021-12-05T03:33:41.088957Z",
          "shell.execute_reply.started": "2021-12-05T03:27:46.112841Z",
          "shell.execute_reply": "2021-12-05T03:33:41.088274Z"
        },
        "trusted": true,
        "id": "C4bCxtAxB1gR"
      },
      "source": [
        "### CNN Training\n",
        "epochs = 80\n",
        "loss_list = []\n",
        "\n",
        "CNN_model = CNN_Net().to(device)\n",
        "optimizer = torch.optim.Adam(CNN_model.parameters(),lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  loss_ = 0\n",
        "  for i, (data, labels) in tqdm(enumerate(data_train)):\n",
        "    x_train = data.float().to(device)\n",
        "    y_train = labels.long().to(device)\n",
        "    x_train = x_train.reshape(x_train.shape[0],1,32,32)\n",
        "    loss, output = train(CNN_model , x_train,y_train,optimizer,criterion)\n",
        "\n",
        "    loss_ += loss.item()\n",
        "  print(\"Epoch {}, Training loss:{}\".format(epoch, loss_ / len(data_train)))\n",
        "  loss_list.append(loss_ / len(data_train))\n",
        " \n",
        "save_model(CNN_model,\"Simple-CNN\")\n",
        "\n",
        "plt.plot(np.arange(epochs),loss_list)\n",
        "plt.title(\"Epochs vs Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmBXkfdTB1gS"
      },
      "source": [
        "### ResNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-05T18:55:14.069106Z",
          "iopub.execute_input": "2021-12-05T18:55:14.069359Z",
          "iopub.status.idle": "2021-12-05T18:55:14.088479Z",
          "shell.execute_reply.started": "2021-12-05T18:55:14.069330Z",
          "shell.execute_reply": "2021-12-05T18:55:14.085555Z"
        },
        "trusted": true,
        "id": "iNi8cKuBB1gS"
      },
      "source": [
        "##Resnet\n",
        "def conv3x3(in_channels, out_channels, stride=1):\n",
        "    return nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
        "                    stride=stride, padding=1, bias=False)\n",
        "# Residual block\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(in_channels, out_channels, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(out_channels, out_channels)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.downsample = downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        if self.downsample:\n",
        "            residual = self.downsample(x)\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "# ResNet\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, layers, num_classes=8):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 16\n",
        "        self.conv = conv3x3(1, 16)\n",
        "        self.bn = nn.BatchNorm2d(16)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self.make_layer(block, 16, layers[0])\n",
        "        self.layer2 = self.make_layer(block, 32, layers[0], 2)\n",
        "        self.layer3 = self.make_layer(block, 64, layers[1], 2)\n",
        "        self.avg_pool = nn.AvgPool2d(8)\n",
        "        self.fc = nn.Linear(64, num_classes)\n",
        "\n",
        "    def make_layer(self, block, out_channels, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if (stride != 1) or (self.in_channels != out_channels):\n",
        "            downsample = nn.Sequential(\n",
        "                conv3x3(self.in_channels, out_channels, stride=stride),\n",
        "                nn.BatchNorm2d(out_channels))\n",
        "        layers = []\n",
        "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
        "        self.in_channels = out_channels\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(out_channels, out_channels))\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)\n",
        "        out = self.bn(out)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.avg_pool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-05T18:55:14.907632Z",
          "iopub.execute_input": "2021-12-05T18:55:14.907969Z",
          "iopub.status.idle": "2021-12-05T18:55:14.939625Z",
          "shell.execute_reply.started": "2021-12-05T18:55:14.907928Z",
          "shell.execute_reply": "2021-12-05T18:55:14.938965Z"
        },
        "trusted": true,
        "id": "BiEKj1N9B1gS"
      },
      "source": [
        "net_args = {\n",
        "    \"block\": ResidualBlock,\n",
        "    \"layers\": [2,1, 1, 2],\n",
        "    \"num_classes\": 8\n",
        "}\n",
        "Resnet_model = ResNet(**net_args).to(device)\n",
        "print(Resnet_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-05T03:34:57.042026Z",
          "iopub.execute_input": "2021-12-05T03:34:57.042310Z",
          "iopub.status.idle": "2021-12-05T03:47:20.219093Z",
          "shell.execute_reply.started": "2021-12-05T03:34:57.042277Z",
          "shell.execute_reply": "2021-12-05T03:47:20.217787Z"
        },
        "trusted": true,
        "id": "RE_0FYcLB1gT"
      },
      "source": [
        "### ResNet Training\n",
        "epochs = 80\n",
        "loss_list = []\n",
        "net_args = {\n",
        "    \"block\": ResidualBlock,\n",
        "    \"layers\": [2,1, 1, 2],\n",
        "    \"num_classes\": 8\n",
        "}\n",
        "Resnet_model = ResNet(**net_args).to(device)\n",
        "optimizer = torch.optim.Adam(Resnet_model.parameters(),lr=0.001)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  loss_ = 0\n",
        "  for i, (data, labels) in tqdm(enumerate(data_train)):\n",
        "    x_train = data.float().to(device)\n",
        "    y_train = labels.long().to(device)\n",
        "    x_train = x_train.reshape(x_train.shape[0],1,32,32)\n",
        "    #print(labels)\n",
        "\n",
        "    loss, output = train(Resnet_model , x_train,y_train,optimizer,criterion)\n",
        "\n",
        "    loss_ += loss.item()\n",
        "  print(\"Epoch {}, Training loss:{}\".format(epoch, loss_ / len(data_train)))\n",
        "  loss_list.append(loss_ / len(data_train))\n",
        "    \n",
        "save_model(Resnet_model,\"ResNet-custom\")\n",
        "  \n",
        "plt.plot(np.arange(epochs),loss_list)\n",
        "plt.title(\"Epochs vs Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4k_1kwzB1gT"
      },
      "source": [
        "### Dense Nets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-05T18:56:21.499998Z",
          "iopub.execute_input": "2021-12-05T18:56:21.500266Z",
          "iopub.status.idle": "2021-12-05T18:56:21.520013Z",
          "shell.execute_reply.started": "2021-12-05T18:56:21.500236Z",
          "shell.execute_reply": "2021-12-05T18:56:21.519307Z"
        },
        "trusted": true,
        "id": "9wfdF34fB1gT"
      },
      "source": [
        "##Dense Net custom\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 1\n",
        "    \n",
        "    def __init__(self, in_channels, growth_rate):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        zip_channels = self.expansion * growth_rate\n",
        "        self.features = nn.Sequential(\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(in_channels, zip_channels, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(zip_channels),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(zip_channels, growth_rate, kernel_size=3, padding=1, bias=False)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = torch.cat([out, x], 1)\n",
        "        return out       \n",
        "         \n",
        "class Transition(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(Transition, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
        "            nn.AvgPool2d(2)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        return out\n",
        "\n",
        "class DenseNet(nn.Module):\n",
        "    \n",
        "    def __init__(self, num_blocks, growth_rate=8, reduction=0.5, num_classes=8):\n",
        "        super(DenseNet, self).__init__()\n",
        "        self.growth_rate = growth_rate\n",
        "        self.reduction = reduction\n",
        "        \n",
        "        num_channels = 2 * growth_rate\n",
        "        \n",
        "        self.features = nn.Conv2d(1, num_channels, kernel_size=3, padding=1, bias=False)\n",
        "        self.layer1, num_channels = self._make_dense_layer(num_channels, num_blocks[0])\n",
        "        self.layer2, num_channels = self._make_dense_layer(num_channels, num_blocks[1])\n",
        "        self.layer3, num_channels = self._make_dense_layer(num_channels, num_blocks[2])\n",
        "        self.layer4, num_channels = self._make_dense_layer(num_channels, num_blocks[3], transition=False)\n",
        "        self.avg_pool = nn.Sequential(\n",
        "            nn.BatchNorm2d(num_channels),\n",
        "            nn.ReLU(True),\n",
        "            nn.AvgPool2d(4),\n",
        "        )\n",
        "        self.classifier = nn.Linear(num_channels, num_classes)\n",
        "        \n",
        "        self._initialize_weight()\n",
        "        \n",
        "    def _make_dense_layer(self, in_channels, nblock, transition=True):\n",
        "        layers = []\n",
        "        for i in range(nblock):\n",
        "            layers += [Bottleneck(in_channels, self.growth_rate)]\n",
        "            in_channels += self.growth_rate\n",
        "        out_channels = in_channels\n",
        "        if transition:\n",
        "            out_channels = int(math.floor(in_channels * self.reduction))\n",
        "            layers += [Transition(in_channels, out_channels)]\n",
        "        return nn.Sequential(*layers), out_channels\n",
        "    \n",
        "    def _initialize_weight(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight.data)\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = self.avg_pool(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "    \n",
        "def DenseNet_custom():\n",
        "    return DenseNet([3,3,3,3], growth_rate=32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-05T18:56:36.528159Z",
          "iopub.execute_input": "2021-12-05T18:56:36.528758Z",
          "iopub.status.idle": "2021-12-05T18:56:36.555471Z",
          "shell.execute_reply.started": "2021-12-05T18:56:36.528722Z",
          "shell.execute_reply": "2021-12-05T18:56:36.554737Z"
        },
        "trusted": true,
        "id": "a6vbZZOnB1gT"
      },
      "source": [
        "Dense_net = DenseNet_custom().to(device)\n",
        "print(Dense_net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-05T03:48:31.726616Z",
          "iopub.execute_input": "2021-12-05T03:48:31.726893Z",
          "iopub.status.idle": "2021-12-05T04:14:37.563706Z",
          "shell.execute_reply.started": "2021-12-05T03:48:31.726862Z",
          "shell.execute_reply": "2021-12-05T04:14:37.562929Z"
        },
        "trusted": true,
        "id": "AtT3QLXdB1gU"
      },
      "source": [
        "## dense net train\n",
        "epochs = 80\n",
        "loss_list = []\n",
        "\n",
        "model_dense_net = DenseNet_custom().to(device)\n",
        "optimizer = torch.optim.Adam(model_dense_net.parameters(), lr=0.001)\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  loss_ = 0\n",
        "  for i, (data, labels) in tqdm(enumerate(data_train)):\n",
        "    x_train = data.float().to(device)\n",
        "    y_train = labels.long().to(device)\n",
        "    x_train = x_train.reshape(x_train.shape[0],1,32,32)\n",
        "    #print(x_train.shape)\n",
        "    loss, output = train(model_dense_net , x_train,y_train,optimizer,criterion)\n",
        "\n",
        "    loss_ += loss.item()\n",
        "  print(\"Epoch {}, Training loss:{}\".format(epoch, loss_ / len(data_train)))\n",
        "  loss_list.append(loss_ / len(data_train))\n",
        "\n",
        "\n",
        "save_model(model_dense_net,\"DenseNet-custom\")\n",
        "\n",
        "plt.plot(np.arange(epochs),loss_list)\n",
        "plt.title(\"Epochs vs Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-04T23:51:20.790626Z",
          "iopub.execute_input": "2021-12-04T23:51:20.790886Z",
          "iopub.status.idle": "2021-12-04T23:51:20.806697Z",
          "shell.execute_reply.started": "2021-12-04T23:51:20.790856Z",
          "shell.execute_reply": "2021-12-04T23:51:20.805969Z"
        },
        "trusted": true,
        "id": "y8pK3yc8B1gU"
      },
      "source": [
        "# DNN_Model_from_directory = DNN_Net().to(device)\n",
        "# DNN_Model_from_directory.load_state_dict(torch.load(\"./DNN_Custom1.pth\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXDkcGkoB1gU"
      },
      "source": [
        "### Shuffle Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-05T04:15:53.887988Z",
          "iopub.execute_input": "2021-12-05T04:15:53.888482Z",
          "iopub.status.idle": "2021-12-05T04:15:53.943735Z",
          "shell.execute_reply.started": "2021-12-05T04:15:53.888441Z",
          "shell.execute_reply": "2021-12-05T04:15:53.943069Z"
        },
        "trusted": true,
        "id": "J2AH32cBB1gU"
      },
      "source": [
        "shufflenet = models.shufflenet_v2_x1_0().to(device)\n",
        "shufflenet.conv1[0] = nn.Conv2d(1, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
        "shufflenet.fc = nn.Linear(in_features=1024, out_features=8, bias=True)\n",
        "shufflenet = shufflenet.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-05T04:15:54.736817Z",
          "iopub.execute_input": "2021-12-05T04:15:54.737310Z",
          "iopub.status.idle": "2021-12-05T04:15:54.744580Z",
          "shell.execute_reply.started": "2021-12-05T04:15:54.737272Z",
          "shell.execute_reply": "2021-12-05T04:15:54.743447Z"
        },
        "trusted": true,
        "id": "Yghv4xTmB1gU"
      },
      "source": [
        "print(shufflenet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-05T04:16:03.911377Z",
          "iopub.execute_input": "2021-12-05T04:16:03.911644Z",
          "iopub.status.idle": "2021-12-05T05:00:47.895953Z",
          "shell.execute_reply.started": "2021-12-05T04:16:03.911615Z",
          "shell.execute_reply": "2021-12-05T05:00:47.894638Z"
        },
        "trusted": true,
        "id": "28JKdUBJB1gV"
      },
      "source": [
        "shufflenet = shufflenet.to(device)\n",
        "epochs = 80\n",
        "loss_list = []\n",
        "\n",
        "optimizer = torch.optim.Adam(shufflenet.parameters(), lr=0.001)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  loss_ = 0\n",
        "  for i, (data, labels) in tqdm(enumerate(data_train)):\n",
        "    x_train = data.float().to(device)\n",
        "    y_train = labels.long().to(device)\n",
        "    x_train = x_train.reshape(x_train.shape[0],1,32,32)\n",
        "    #print(x_train.shape)\n",
        "\n",
        "    loss, output = train(shufflenet,x_train,y_train,optimizer,criterion)\n",
        "\n",
        "    loss_ += loss.item()\n",
        "  print(\"Epoch {}, Training loss:{}\".format(epoch, loss_ / len(data_train)))\n",
        "  loss_list.append(loss_ / len(data_train))\n",
        "    \n",
        "save_model(shufflenet,\"Shuffle-Net\")\n",
        "\n",
        "plt.plot(np.arange(epochs),loss_list)\n",
        "plt.title(\"Epochs vs Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbfOfFcKB1gV"
      },
      "source": [
        "### Mobile Net V3 Small"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-05T18:59:16.794940Z",
          "iopub.execute_input": "2021-12-05T18:59:16.795221Z",
          "iopub.status.idle": "2021-12-05T18:59:16.868503Z",
          "shell.execute_reply.started": "2021-12-05T18:59:16.795189Z",
          "shell.execute_reply": "2021-12-05T18:59:16.867815Z"
        },
        "trusted": true,
        "id": "k3ThhB4wB1gV"
      },
      "source": [
        "mobilenet_v3_small = models.mobilenet_v3_small()\n",
        "mobilenet_v3_small.features[0][0] = nn.Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
        "mobilenet_v3_small.classifier[3] = nn.Linear(in_features=1024, out_features=8, bias=True)\n",
        "mobilenet_v3_small = mobilenet_v3_small.to(device)\n",
        "#summary(mobilenet_v3_small,(1,32,32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-05T18:59:19.456730Z",
          "iopub.execute_input": "2021-12-05T18:59:19.456996Z",
          "iopub.status.idle": "2021-12-05T18:59:19.467375Z",
          "shell.execute_reply.started": "2021-12-05T18:59:19.456965Z",
          "shell.execute_reply": "2021-12-05T18:59:19.466619Z"
        },
        "trusted": true,
        "id": "c-nFDqBrB1gV"
      },
      "source": [
        "mobilenet_v3_small"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-04T19:29:46.705267Z",
          "iopub.execute_input": "2021-12-04T19:29:46.706403Z",
          "iopub.status.idle": "2021-12-04T19:58:19.313539Z",
          "shell.execute_reply.started": "2021-12-04T19:29:46.706353Z",
          "shell.execute_reply": "2021-12-04T19:58:19.312263Z"
        },
        "trusted": true,
        "id": "mjYhoIhkB1gW"
      },
      "source": [
        "epochs = 40\n",
        "loss_list = []\n",
        "\n",
        "optimizer = torch.optim.Adam(mobilenet_v3_small.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
        "#optimizer = torch.optim.SGD(mobilenet_v3_small.parameters(), lr=0.001)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  loss_ = 0\n",
        "  for i, (data, labels) in tqdm(enumerate(data_train)):\n",
        "    x_train = data.float().to(device)\n",
        "    y_train = labels.long().to(device)\n",
        "    x_train = x_train.reshape(x_train.shape[0],1,32,32)\n",
        "    #print(x_train.shape)\n",
        "\n",
        "    loss, output = train(mobilenet_v3_small , x_train,y_train,optimizer,criterion)\n",
        "\n",
        "    loss_ += loss.item()\n",
        "  print(\"Epoch {}, Training loss:{}\".format(epoch, loss_ / len(data_train)))\n",
        "  loss_list.append(loss_ / len(data_train))\n",
        "    \n",
        "\n",
        "plt.plot(np.arange(epochs),loss_list)\n",
        "plt.title(\"Epochs vs Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSlPwNbfB1gW"
      },
      "source": [
        "### Squeeze Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-04T18:07:19.394603Z",
          "iopub.execute_input": "2021-12-04T18:07:19.394878Z",
          "iopub.status.idle": "2021-12-04T18:07:19.420135Z",
          "shell.execute_reply.started": "2021-12-04T18:07:19.394843Z",
          "shell.execute_reply": "2021-12-04T18:07:19.418059Z"
        },
        "trusted": true,
        "id": "jrJ_xzUEB1gW"
      },
      "source": [
        "#squeeze net\n",
        "import torch.nn.init as init\n",
        "class Fire(nn.Module):\n",
        "    def __init__(self, inplanes: int, squeeze_planes: int, expand1x1_planes: int, expand3x3_planes: int) -> None:\n",
        "        super().__init__()\n",
        "        self.inplanes = inplanes\n",
        "        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n",
        "        self.squeeze_activation = nn.ReLU(inplace=True)\n",
        "        self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes, kernel_size=1)\n",
        "        self.expand1x1_activation = nn.ReLU(inplace=True)\n",
        "        self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes, kernel_size=3, padding=1)\n",
        "        self.expand3x3_activation = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.squeeze_activation(self.squeeze(x))\n",
        "        return torch.cat(\n",
        "            [self.expand1x1_activation(self.expand1x1(x)), self.expand3x3_activation(self.expand3x3(x))], 1\n",
        "        )\n",
        "\n",
        "class SqueezeNet(nn.Module):\n",
        "    def __init__(self, version: str = \"1_0\", num_classes: int = 8, dropout: float = 0.1) -> None:\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        if version == \"1_0\":\n",
        "            self.features = nn.Sequential(\n",
        "                nn.Conv2d(1, 96, kernel_size=7, stride=2),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(96, 16, 64, 64),\n",
        "                Fire(128, 16, 64, 64),\n",
        "                Fire(128, 32, 128, 128),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(256, 32, 128, 128),\n",
        "                Fire(256, 48, 192, 192),\n",
        "                Fire(384, 48, 192, 192),\n",
        "                Fire(384, 64, 256, 256),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(512, 64, 256, 256),\n",
        "            )\n",
        "        elif version == \"1_1\":\n",
        "            self.features = nn.Sequential(\n",
        "                nn.Conv2d(1, 64, kernel_size=3, stride=2),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(64, 16, 64, 64),\n",
        "                Fire(128, 16, 64, 64),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(128, 32, 128, 128),\n",
        "                Fire(256, 32, 128, 128),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
        "                Fire(256, 48, 192, 192),\n",
        "                Fire(384, 48, 192, 192),\n",
        "                Fire(384, 64, 256, 256),\n",
        "                Fire(512, 64, 256, 256),\n",
        "            )\n",
        "        else:\n",
        "            # FIXME: Is this needed? SqueezeNet should only be called from the\n",
        "            # FIXME: squeezenet1_x() functions\n",
        "            # FIXME: This checking is not done for the other models\n",
        "            raise ValueError(f\"Unsupported SqueezeNet version {version}: 1_0 or 1_1 expected\")\n",
        "\n",
        "        # Final convolution is initialized differently from the rest\n",
        "        final_conv = nn.Conv2d(512, self.num_classes, kernel_size=1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=dropout), final_conv, nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                if m is final_conv:\n",
        "                    init.normal_(m.weight, mean=0.0, std=0.01)\n",
        "                else:\n",
        "                    init.kaiming_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        return x\n",
        "\n",
        "\n",
        "def _squeezenet(version: str, pretrained: bool, progress: bool, **kwargs: Any) -> SqueezeNet:\n",
        "    model = SqueezeNet(version, **kwargs)\n",
        "    if pretrained:\n",
        "        arch = \"squeezenet\" + version\n",
        "        state_dict = load_state_dict_from_url(model_urls[arch], progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def squeezenet1_0(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> SqueezeNet:\n",
        "    r\"\"\"SqueezeNet model architecture from the `\"SqueezeNet: AlexNet-level\n",
        "    accuracy with 50x fewer parameters and <0.5MB model size\"\n",
        "    <https://arxiv.org/abs/1602.07360>`_ paper.\n",
        "    The required minimum input size of the model is 21x21.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet(\"1_0\", pretrained, progress, **kwargs)\n",
        "\n",
        "def squeezenet1_1(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> SqueezeNet:\n",
        "    r\"\"\"SqueezeNet model architecture from the `\"SqueezeNet: AlexNet-level\n",
        "    accuracy with 50x fewer parameters and <0.5MB model size\"\n",
        "    <https://arxiv.org/abs/1602.07360>`_ paper.\n",
        "    The required minimum input size of the model is 21x21.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return _squeezenet(\"1_1\", pretrained, progress, **kwargs)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-04T18:08:18.475773Z",
          "iopub.execute_input": "2021-12-04T18:08:18.476147Z",
          "iopub.status.idle": "2021-12-04T18:23:36.161223Z",
          "shell.execute_reply.started": "2021-12-04T18:08:18.476116Z",
          "shell.execute_reply": "2021-12-04T18:23:36.160478Z"
        },
        "trusted": true,
        "id": "XlonP7W5B1gW"
      },
      "source": [
        "epochs = 80\n",
        "loss_list = []\n",
        "\n",
        "model_squeeze = squeezenet1_1(num_classes=8).to(device)\n",
        "\n",
        "#optimizer = torch.optim.Adam(model_dense_net.parameters(), lr=0.0001)#, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  loss_ = 0\n",
        "  for i, (data, labels) in tqdm(enumerate(data_train)):\n",
        "    x_train = data.float().to(device)\n",
        "    y_train = labels.long().to(device)\n",
        "    x_train = x_train.reshape(x_train.shape[0],1,32,32)\n",
        "    #print(x_train.shape)\n",
        "\n",
        "    loss, output = train(model_squeeze , x_train,y_train,optimizer,criterion)\n",
        "\n",
        "    loss_ += loss.item()\n",
        "  print(\"Epoch {}, Training loss:{}\".format(epoch, loss_ / len(data_train)))\n",
        "  loss_list.append(loss_ / len(data_train))\n",
        "    \n",
        "\n",
        "plt.plot(np.arange(epochs),loss_list)\n",
        "plt.title(\"Epochs vs Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93bUtRkUB1gW"
      },
      "source": [
        "### XGboost Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJYASaAjB1gW"
      },
      "source": [
        "xgb=XGBClassifier()\n",
        "xgb.fit(X_train,Y_train)\n",
        "\n",
        "import pickle\n",
        "file_name = \"xgboost-trained-1.pkl\"\n",
        "\n",
        "# save\n",
        "pickle.dump(xgb, open(file_name, \"wb\"))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFzEIvF_B1gX"
      },
      "source": [
        "### LightGBM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38aKiVhlB1gX"
      },
      "source": [
        "import lightgbm as lgb\n",
        "clf = lgb.LGBMClassifier(n_estimators=40,max_depth = 3)\n",
        "clf.fit(X_train,Y_train)\n",
        "\n",
        "file_name = \"lightgbm-trained-1.pkl\"\n",
        "\n",
        "# save\n",
        "pickle.dump(clf, open(file_name, \"wb\"))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jb7YwsK1B1gX"
      },
      "source": [
        "### LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-05T19:01:00.247444Z",
          "iopub.execute_input": "2021-12-05T19:01:00.247705Z",
          "iopub.status.idle": "2021-12-05T19:01:00.258936Z",
          "shell.execute_reply.started": "2021-12-05T19:01:00.247676Z",
          "shell.execute_reply": "2021-12-05T19:01:00.257979Z"
        },
        "trusted": true,
        "id": "U-sgU-sBB1gX"
      },
      "source": [
        "##LSTM Model\n",
        "class LSTM_Model(nn.Module):\n",
        "    def __init__(self, input_size,hidden_size, layer_size, output_size, bidirectional=True):\n",
        "        super(LSTM_Model,self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.layer_size = layer_size\n",
        "        self.output_size = output_size\n",
        "        self.bidirectional = True\n",
        "        \n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, layer_size, batch_first=True, bidirectional=bidirectional)\n",
        "        \n",
        "        if bidirectional:\n",
        "            self.layer1 = nn.Linear(hidden_size*2,128)\n",
        "            self.layer2 = nn.Linear(128,output_size)\n",
        "#         else:\n",
        "#             self.layer = nn.Linear(hidden_size, output_size)\n",
        "        \n",
        "        # FNN\n",
        "    def forward(self, images, prints=False):\n",
        "        \n",
        "        if self.bidirectional:\n",
        "            # Hidden state:\n",
        "            hidden_state = torch.zeros(self.layer_size*2, images.size(0), self.hidden_size).to(device)\n",
        "            # Cell state:\n",
        "            cell_state = torch.zeros(self.layer_size*2, images.size(0), self.hidden_size).to(device)\n",
        "        else:\n",
        "            # Hidden state:\n",
        "            hidden_state = torch.zeros(self.layer_size, images.size(0), self.hidden_size).to(device)\n",
        "            # Cell state:\n",
        "            cell_state = torch.zeros(self.layer_size, images.size(0), self.hidden_size).to(device)\n",
        "        \n",
        "        # LSTM:\n",
        "        output, (last_hidden_state, last_cell_state) = self.lstm(images, (hidden_state, cell_state))\n",
        "       \n",
        "        # Reshape\n",
        "        output = output[:, -1, :]\n",
        "        if prints: print('output reshape:', output.shape)\n",
        "        \n",
        "        # FNN:\n",
        "        output = self.layer1(output)\n",
        "        output = self.layer2(output)\n",
        "        \n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-05T19:01:24.077066Z",
          "iopub.execute_input": "2021-12-05T19:01:24.077369Z",
          "iopub.status.idle": "2021-12-05T19:01:24.081759Z",
          "shell.execute_reply.started": "2021-12-05T19:01:24.077339Z",
          "shell.execute_reply": "2021-12-05T19:01:24.080980Z"
        },
        "trusted": true,
        "id": "w_fL7knlB1gX"
      },
      "source": [
        "print(LSTM_model_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-12-05T19:01:16.627904Z",
          "iopub.execute_input": "2021-12-05T19:01:16.628610Z",
          "iopub.status.idle": "2021-12-05T19:01:16.633843Z",
          "shell.execute_reply.started": "2021-12-05T19:01:16.628572Z",
          "shell.execute_reply": "2021-12-05T19:01:16.632932Z"
        },
        "trusted": true,
        "id": "tjb4gF49B1gY"
      },
      "source": [
        "batch_size  = Batch\n",
        "input_size  = 32\n",
        "hidden_size = 128\n",
        "layer_size  = 3\n",
        "output_size = 8\n",
        "\n",
        "epochs = 80\n",
        "loss_list = []\n",
        "\n",
        "LSTM_model_train = LSTM_Model(input_size, hidden_size, layer_size, output_size).to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(LSTM_model_train.parameters(), lr=0.001)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  loss_ = 0\n",
        "  for i, (data, labels) in tqdm(enumerate(data_train)):\n",
        "    x_train = data.float().to(device)\n",
        "    y_train = labels.long().to(device)\n",
        "    x_train = x_train.reshape(x_train.shape[0],32,32)\n",
        "    #print(x_train.shape)\n",
        "\n",
        "    loss, output = train(LSTM_model_train , x_train,y_train,optimizer,criterion)\n",
        "\n",
        "    loss_ += loss.item()\n",
        "  print(\"Epoch {}, Training loss:{}\".format(epoch, loss_ / len(data_train)))\n",
        "  loss_list.append(loss_ / len(data_train))\n",
        "    \n",
        "save_model(LSTM_model_train,\"LSTM-Model\")\n",
        "\n",
        "plt.plot(np.arange(epochs),loss_list)\n",
        "plt.title(\"Epochs vs Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDNeNvj0B1gY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}